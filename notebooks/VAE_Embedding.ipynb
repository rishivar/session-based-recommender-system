{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "      <th>896</th>\n",
       "      <th>897</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23686</th>\n",
       "      <td>28139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23687</th>\n",
       "      <td>28140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23688</th>\n",
       "      <td>28141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23689</th>\n",
       "      <td>28142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23690</th>\n",
       "      <td>28143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23691 rows Ã— 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  0  1  2  3  4  5  6  7  8  ...  894  895  896  897  898  899  \\\n",
       "0            2  1  1  1  1  1  1  1  1  1  ...    0    0    0    0    0    0   \n",
       "1            3  1  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "2            4  1  0  0  0  1  0  0  0  1  ...    0    0    0    0    0    0   \n",
       "3            7  0  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "4            8  1  0  1  0  1  0  0  0  1  ...    0    0    0    0    0    0   \n",
       "...        ... .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "23686    28139  1  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "23687    28140  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "23688    28141  1  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "23689    28142  0  0  0  0  1  1  0  0  0  ...    0    0    0    0    0    0   \n",
       "23690    28143  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "\n",
       "       900  901  902  903  \n",
       "0        0    0    0    0  \n",
       "1        0    0    0    0  \n",
       "2        0    0    0    0  \n",
       "3        0    0    0    0  \n",
       "4        0    0    0    0  \n",
       "...    ...  ...  ...  ...  \n",
       "23686    0    0    0    0  \n",
       "23687    0    0    0    0  \n",
       "23688    0    0    0    0  \n",
       "23689    0    0    0    0  \n",
       "23690    0    0    0    0  \n",
       "\n",
       "[23691 rows x 905 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_attributes = pd.read_csv(\"../dataset/dressipi_recsys2022/item_attributes.csv\")\n",
    "item_attributes = item_attributes[item_attributes.columns[:905]]\n",
    "item_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23691"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(item_attributes['item_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(keras.Model):\n",
    "    def __init__(self, inputShape, batchSize, latentSize):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.inputShape = inputShape\n",
    "        self.batchSize = batchSize\n",
    "        self.latentSize = latentSize\n",
    "        self.input_layer = keras.Input(shape=self.inputShape)\n",
    "        self.e2 = keras.layers.Dense(units=self.latentSize*4)(self.input_layer)\n",
    "        self.b2 = tf.keras.layers.BatchNormalization()(self.e2)\n",
    "        self.r2 = tf.keras.layers.LeakyReLU(alpha=0.3)(self.b2)\n",
    "        self.d2 = tf.keras.layers.Dropout(0.2, seed = 42)(self.r2)\n",
    "        self.z_mean = keras.layers.Dense(self.latentSize)(self.d2)\n",
    "        self.z_log_sigma = keras.layers.Dense(self.latentSize)(self.d2)\n",
    "        self.z = keras.layers.Lambda(self.sampling)([self.z_mean, self.z_log_sigma])\n",
    "        self.encoder = keras.Model(self.input_layer, [self.z_mean, self.z_log_sigma, self.z], name='encoder')\n",
    "        self.latent_inputs = keras.Input(shape=(self.latentSize,), name='z_sampling')\n",
    "        self.e5 = keras.layers.Dense(units=self.latentSize*4)(self.latent_inputs)\n",
    "        self.b5 = tf.keras.layers.BatchNormalization()(self.e5)\n",
    "        self.r5 = tf.keras.layers.LeakyReLU(alpha=0.3)(self.b5)\n",
    "        self.d5 = tf.keras.layers.Dropout(0.2, seed = 42)(self.r5)\n",
    "        self.output_layer = keras.layers.Dense(self.inputShape[0], activation='sigmoid')(self.d5)\n",
    "        self.decoder = keras.Model(self.latent_inputs, self.output_layer, name='decoder')\n",
    "        self.output_layer = self.decoder(self.encoder(self.input_layer)[2])\n",
    "        self.vae = keras.Model(self.input_layer, self.output_layer, name='vae_mlp')\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latentSize),\n",
    "                              mean=0., stddev=0.1)\n",
    "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "    def call(self, vector):\n",
    "        z_mean, z_log_sigma, z = self.encoder(vector)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
    "        return reconstructed\n",
    "\n",
    "    def createEmbedding(self, vector):\n",
    "        return self.encoder(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = item_attributes[item_attributes.columns[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 17:31:03.414425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "autoEncoder = VariationalAutoEncoder(inputShape=(dataset.shape[1],),\n",
    "                                     batchSize=32,\n",
    "                                     latentSize=64)\n",
    "autoEncoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([23691, 904])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tensor = tf.convert_to_tensor(dataset)\n",
    "dataset_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1, cooldown=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "593/593 [==============================] - 3s 3ms/step - loss: 0.1041 - kl_loss: 0.0815 - val_loss: 0.0180 - val_kl_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0190 - kl_loss: 0.0063 - val_loss: 0.0134 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0142 - kl_loss: 0.0027 - val_loss: 0.0128 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0133 - kl_loss: 0.0024 - val_loss: 0.0123 - val_kl_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0130 - kl_loss: 0.0025 - val_loss: 0.0122 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0129 - kl_loss: 0.0027 - val_loss: 0.0121 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0127 - kl_loss: 0.0027 - val_loss: 0.0119 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0126 - kl_loss: 0.0028 - val_loss: 0.0117 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0125 - kl_loss: 0.0028 - val_loss: 0.0116 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0124 - kl_loss: 0.0028 - val_loss: 0.0116 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0123 - kl_loss: 0.0028 - val_loss: 0.0115 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0122 - kl_loss: 0.0028 - val_loss: 0.0113 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0121 - kl_loss: 0.0028 - val_loss: 0.0113 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0120 - kl_loss: 0.0029 - val_loss: 0.0113 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0119 - kl_loss: 0.0028 - val_loss: 0.0112 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0119 - kl_loss: 0.0028 - val_loss: 0.0111 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0118 - kl_loss: 0.0029 - val_loss: 0.0111 - val_kl_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0118 - kl_loss: 0.0029 - val_loss: 0.0111 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0110 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0110 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0115 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "578/593 [============================>.] - ETA: 0s - loss: 0.0115 - kl_loss: 0.0029\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0115 - kl_loss: 0.0029 - val_loss: 0.0108 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0114 - kl_loss: 0.0029 - val_loss: 0.0107 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0107 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "582/593 [============================>.] - ETA: 0s - loss: 0.0113 - kl_loss: 0.0029\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 34/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 35/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 36/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 37/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 38/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 39/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 40/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 41/200\n",
      "579/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
      "Epoch 42/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 43/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 44/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 45/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 46/200\n",
      "579/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 47/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 48/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 49/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 50/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 51/200\n",
      "575/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 52/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 53/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 54/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 55/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 56/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 57/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
      "Epoch 58/200\n",
      "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad4d7ab1c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoEncoder.fit(dataset_tensor, dataset_tensor,\n",
    "                epochs=200,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[callbacks],\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([23691, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = autoEncoder.createEmbedding(dataset_tensor)[2]\n",
    "#encoded_dataset = np.hstack([encoded_dataset[0], encoded_dataset[1]])\n",
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(23691, 64), dtype=float32, numpy=\n",
       "array([[-0.04274673,  0.04315985,  0.00921491, ..., -0.03659924,\n",
       "        -0.17490537,  0.16326496],\n",
       "       [-0.18153004,  0.04759347, -0.00341143, ..., -0.12464795,\n",
       "        -0.04096986,  0.04406041],\n",
       "       [-0.05285801, -0.08981786, -0.2105933 , ..., -0.01881974,\n",
       "        -0.07685136, -0.07413374],\n",
       "       ...,\n",
       "       [-0.05543809,  0.00126155, -0.02400142, ...,  0.02111587,\n",
       "         0.08524208, -0.1505173 ],\n",
       "       [-0.09645688, -0.0298008 , -0.01686037, ...,  0.07884201,\n",
       "         0.05439367, -0.02737273],\n",
       "       [ 0.09505475,  0.07237296,  0.08200808, ...,  0.10972885,\n",
       "         0.102649  ,  0.40639207]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23691, 23691)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.save('../dataset/ICM', encoded_dataset, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code imported from https://github.com/KingPowa/Rec_Sys_2022_Boston_Team"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
