{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KsKT535A29L3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "heBRTx0729L6"
      },
      "outputs": [],
      "source": [
        "item_attributes = pd.read_csv(\"../dataset/dressipi_recsys2022/item_attributes.csv\")\n",
        "item_attributes = item_attributes[item_attributes.columns[:905]]\n",
        "item_attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBjwCe_29L7",
        "outputId": "983eaab0-fae1-4505-a4bd-5fc9c618318e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23691"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(list(item_attributes['item_id'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SNDguVZU29L7"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoEncoder(keras.Model):\n",
        "    def __init__(self, input_shape, latent_size):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.latent_size = latent_size\n",
        "        self.input_layer = tf.keras.Input(shape=self.input_shape)\n",
        "        self.encoder_layer = tf.keras.layers.Dense(units=512)(self.input_layer)\n",
        "        self.encoder_layer = tf.keras.layers.BatchNormalization()(self.encoder_layer)\n",
        "        self.encoder_layer = tf.keras.layers.LeakyReLU()(self.encoder_layer)\n",
        "        self.encoder_layer = tf.keras.layers.Dense(units=256)(self.input_layer)\n",
        "        self.encoder_layer = tf.keras.layers.BatchNormalization()(self.encoder_layer)\n",
        "        self.encoder_layer = tf.keras.layers.LeakyReLU()(self.encoder_layer)\n",
        "        self.encoder_layer = tf.keras.layers.Dense(units=128)(self.input_layer)\n",
        "        self.encoder_layer = tf.keras.layers.BatchNormalization()(self.encoder_layer)\n",
        "        self.encoder_layer = tf.keras.layers.LeakyReLU()(self.encoder_layer)\n",
        "        self.z_mean = tf.keras.layers.Dense(self.latent_size)(self.encoder_layer)\n",
        "        self.z_sigma = tf.keras.layers.Dense(self.latent_size)(self.encoder_layer)\n",
        "        self.z = tf.keras.layers.Lambda(self.sampling)([self.z_mean, self.z_sigma])\n",
        "        self.encoder = tf.keras.Model(self.input_layer, [self.z_mean, self.z_sigma, self.z], name='encoder')\n",
        "        self.latent_inputs = tf.keras.Input(shape=(self.latent_size,), name='z_sampling')\n",
        "        self.decoder_layer = tf.keras.layers.Dense(units=128)(self.latent_inputs)\n",
        "        self.decoder_layer = tf.keras.layers.BatchNormalization()(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.LeakyReLU()(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.Dense(units=256)(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.BatchNormalization()(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.LeakyReLU()(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.Dense(units=512)(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.BatchNormalization()(self.decoder_layer)\n",
        "        self.decoder_layer = tf.keras.layers.LeakyReLU()(self.decoder_layer)\n",
        "        self.output_layer = tf.keras.layers.Dense(self.input_shape[0], activation='sigmoid')(self.decoder_layer)\n",
        "        self.decoder = tf.keras.Model(self.latent_inputs, self.output_layer, name='decoder')\n",
        "        self.output_layer = self.decoder(self.encoder(self.input_layer)[2])\n",
        "        self.vae = keras.Model(self.input_layer, self.output_layer, name='vae')\n",
        "\n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_sigma = args\n",
        "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latentSize),\n",
        "                              mean=0., stddev=0.1)\n",
        "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
        "\n",
        "    def call(self, vector):\n",
        "        z_mean, z_log_sigma, z = self.encoder(vector)\n",
        "        reconstructed = self.decoder(z)\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma) + 1\n",
        "        )\n",
        "        self.add_loss(kl_loss)\n",
        "        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
        "        return reconstructed\n",
        "\n",
        "    def createEmbedding(self, vector):\n",
        "        return self.encoder(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zE8uDf0Z29L8"
      },
      "outputs": [],
      "source": [
        "dataset = item_attributes[item_attributes.columns[1:]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HpNVOPKu29L9",
        "outputId": "471a676c-41e6-4dab-d437-29a4b0e9e792"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-19 17:31:03.414425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "autoEncoder = VariationalAutoEncoder(inputShape=(dataset.shape[1],),\n",
        "                                     latentSize=64)\n",
        "autoEncoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0Tqbuqfk29L-",
        "outputId": "e0ea0fc7-c8a5-451a-e7eb-266709019be4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([23691, 904])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_tensor = tf.convert_to_tensor(dataset)\n",
        "dataset_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HAn2GgLX29L-"
      },
      "outputs": [],
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UrsT1tAQ29L-",
        "outputId": "2a58faca-40ea-49b7-b501-be16ceb19efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "593/593 [==============================] - 3s 3ms/step - loss: 0.1041 - kl_loss: 0.0815 - val_loss: 0.0180 - val_kl_loss: 0.0032 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0190 - kl_loss: 0.0063 - val_loss: 0.0134 - val_kl_loss: 0.0019 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0142 - kl_loss: 0.0027 - val_loss: 0.0128 - val_kl_loss: 0.0021 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0133 - kl_loss: 0.0024 - val_loss: 0.0123 - val_kl_loss: 0.0022 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0130 - kl_loss: 0.0025 - val_loss: 0.0122 - val_kl_loss: 0.0024 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0129 - kl_loss: 0.0027 - val_loss: 0.0121 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0127 - kl_loss: 0.0027 - val_loss: 0.0119 - val_kl_loss: 0.0026 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0126 - kl_loss: 0.0028 - val_loss: 0.0117 - val_kl_loss: 0.0026 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0125 - kl_loss: 0.0028 - val_loss: 0.0116 - val_kl_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0124 - kl_loss: 0.0028 - val_loss: 0.0116 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0123 - kl_loss: 0.0028 - val_loss: 0.0115 - val_kl_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0122 - kl_loss: 0.0028 - val_loss: 0.0113 - val_kl_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0121 - kl_loss: 0.0028 - val_loss: 0.0113 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0120 - kl_loss: 0.0029 - val_loss: 0.0113 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0119 - kl_loss: 0.0028 - val_loss: 0.0112 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0119 - kl_loss: 0.0028 - val_loss: 0.0111 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0118 - kl_loss: 0.0029 - val_loss: 0.0111 - val_kl_loss: 0.0030 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0118 - kl_loss: 0.0029 - val_loss: 0.0111 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0110 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0110 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0117 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0030 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0116 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0115 - kl_loss: 0.0029 - val_loss: 0.0109 - val_kl_loss: 0.0029 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "578/593 [============================>.] - ETA: 0s - loss: 0.0115 - kl_loss: 0.0029\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0115 - kl_loss: 0.0029 - val_loss: 0.0108 - val_kl_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0114 - kl_loss: 0.0029 - val_loss: 0.0107 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 28/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 29/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0107 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 30/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "582/593 [============================>.] - ETA: 0s - loss: 0.0113 - kl_loss: 0.0029\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 34/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 35/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 36/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 37/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 38/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 39/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 40/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 41/200\n",
            "579/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-05\n",
            "Epoch 42/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
            "Epoch 43/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
            "Epoch 44/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
            "Epoch 45/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
            "Epoch 46/200\n",
            "579/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
            "Epoch 47/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 48/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 49/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0105 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 50/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 51/200\n",
            "575/593 [============================>.] - ETA: 0s - loss: 0.0112 - kl_loss: 0.0029\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 52/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0113 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 53/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 54/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 55/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 56/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 57/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n",
            "Epoch 58/200\n",
            "593/593 [==============================] - 2s 3ms/step - loss: 0.0112 - kl_loss: 0.0029 - val_loss: 0.0106 - val_kl_loss: 0.0029 - lr: 1.0000e-07\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad4d7ab1c0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "autoEncoder.fit(dataset_tensor, dataset_tensor,\n",
        "                epochs=200,\n",
        "                batch_size=32,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[callbacks],\n",
        "                verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tJ6hbjG429L_",
        "outputId": "e8cad631-373f-45ec-ff6f-ad796e3977f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([23691, 64])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset = autoEncoder.createEmbedding(dataset_tensor)[2]\n",
        "#encoded_dataset = np.hstack([encoded_dataset[0], encoded_dataset[1]])\n",
        "encoded_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM8vKamQ29L_",
        "outputId": "137dbb49-8ac2-4dff-dcec-41c433545add"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(23691, 64), dtype=float32, numpy=\n",
              "array([[-0.04274673,  0.04315985,  0.00921491, ..., -0.03659924,\n",
              "        -0.17490537,  0.16326496],\n",
              "       [-0.18153004,  0.04759347, -0.00341143, ..., -0.12464795,\n",
              "        -0.04096986,  0.04406041],\n",
              "       [-0.05285801, -0.08981786, -0.2105933 , ..., -0.01881974,\n",
              "        -0.07685136, -0.07413374],\n",
              "       ...,\n",
              "       [-0.05543809,  0.00126155, -0.02400142, ...,  0.02111587,\n",
              "         0.08524208, -0.1505173 ],\n",
              "       [-0.09645688, -0.0298008 , -0.01686037, ...,  0.07884201,\n",
              "         0.05439367, -0.02737273],\n",
              "       [ 0.09505475,  0.07237296,  0.08200808, ...,  0.10972885,\n",
              "         0.102649  ,  0.40639207]], dtype=float32)>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hTip2NSa29MA"
      },
      "outputs": [],
      "source": [
        "np.save('../dataset/ICM', encoded_dataset, allow_pickle=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}